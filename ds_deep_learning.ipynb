{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bba1251c788b599b28545e891482886aaae67913"
   },
   "source": [
    "# Using Deep Learning to Identify Images of Fruits\n",
    "\n",
    "I will be using deep learning techniques via the Keras API to train a Convolutional Neural Network (CNN) model on a dataset of fruit images, and evaluate this model's accuracy on a separate test dataset of fruit images.\n",
    "\n",
    "I will be focusing on optimizing my CNN model to give me the highest possible accuracy on both my training & test data.\n",
    "\n",
    "## Importing Training & Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "  0%|          | 0/166 [00:00<?, ?it/s]/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "100%|██████████| 166/166 [00:00<00:00, 333.20it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 356.82it/s]\n",
      "100%|██████████| 246/246 [00:00<00:00, 356.07it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 354.69it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 362.07it/s]\n",
      "100%|██████████| 165/165 [00:00<00:00, 372.40it/s]\n",
      "100%|██████████| 143/143 [00:00<00:00, 365.08it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 360.89it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 364.63it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 365.54it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 361.89it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 358.32it/s]\n",
      "100%|██████████| 738/738 [00:02<00:00, 354.22it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 361.71it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 363.24it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 360.01it/s]\n",
      "100%|██████████| 427/427 [00:01<00:00, 360.56it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 359.51it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 363.04it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 359.94it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from skimage import io, transform\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import LSTM, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "img_size = 100\n",
    "train_dir = './data/fruits/test/'\n",
    "test_dir =  './data/fruits/train/'\n",
    "\n",
    "def get_data(folder_path):\n",
    "    imgs = []\n",
    "    indices = []\n",
    "    labels = []\n",
    "    for idx, folder_name in enumerate(os.listdir(folder_path)[:10]):\n",
    "        if not folder_name.startswith('.'):\n",
    "            labels.append(folder_name)\n",
    "            for file_name in tqdm(os.listdir(folder_path + folder_name)):\n",
    "                if not file_name.startswith('.'):\n",
    "                    img_file = io.imread(folder_path + folder_name + '/' + file_name)\n",
    "                    if img_file is not None:\n",
    "                        img_file = transform.resize(img_file, (img_size, img_size))\n",
    "                        imgs.append(np.asarray(img_file))\n",
    "                        indices.append(idx)\n",
    "    imgs = np.asarray(imgs)\n",
    "    indices = np.asarray(indices)\n",
    "    labels = np.asarray(labels)\n",
    "    return imgs, indices, labels\n",
    "\n",
    "X_train, y_train, train_labels = get_data(train_dir)\n",
    "X_test, y_test, test_labels = get_data(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "d2b6e53ff4d63ee0800f862a35c37d446260faa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1709, 100, 100, 3)\n",
      "X_test shape: (5093, 100, 100, 3)\n",
      "y_train: [0 0 0 ... 9 9 9]\n",
      "y_test: [0 0 0 ... 9 9 9]\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train:', y_train)\n",
    "print('y_test:', y_test)\n",
    "# print('First image - X_train:', X_train[0])\n",
    "\n",
    "num_categories = len(np.unique(y_train))\n",
    "\n",
    "new_X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3]).astype('float32')\n",
    "new_X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], X_test.shape[3]).astype('float32')\n",
    "new_y_train = keras.utils.to_categorical(y_train, num_categories)\n",
    "new_y_test = keras.utils.to_categorical(y_test, num_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6a733842f97046a9d84d5bfc4f4706112be7089"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "daec20a5cf3d33f9152158f8966550226fe6cd70",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_imgs(folder_path):\n",
    "    for idx, folder_name in enumerate(os.listdir(folder_path)):\n",
    "        if idx % 25 == 0:\n",
    "            if not folder_name.startswith('.'):\n",
    "                for idx2, file_name in enumerate(tqdm(os.listdir(folder_path + folder_name))):\n",
    "                    if idx2 % 75 == 0:\n",
    "                        if not file_name.startswith('.'):\n",
    "                            img_filename = folder_path + folder_name + '/' + file_name\n",
    "                            display(Image(filename=img_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ee4e6062b24df817decc7db886b1191db01c216"
   },
   "source": [
    "### Examples of Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b1e099a114f62d93e16e1e8f67e5fe30e0a8af4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_imgs(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7a8eedd0e7a2a9659aae329fe1f053c4831b882"
   },
   "source": [
    "### Examples of Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "187d471260911d090453ebb7f393e4930e6b5a28",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_imgs(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5ac3fd9818637f6ce628d6abcc3c7c4f8e14bc2"
   },
   "source": [
    "## Initial Model Selection\n",
    "### Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "882e982758253064cc542eb250e78f9f0c0be851",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, batch_size, epochs):\n",
    "    history = model.fit(new_X_train, new_y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(new_X_test, new_y_test))\n",
    "    score = model.evaluate(new_X_test, new_y_test, verbose=0)\n",
    "    print('***Metrics Names***', model.metrics_names)\n",
    "    print('***Metrics Values***', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa628de94ca588d5321883962c3493577b8ded0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 98, 98, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 96, 96, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 147456)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               18874496  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 18,895,178\n",
      "Trainable params: 18,895,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1709 samples, validate on 5093 samples\n",
      "Epoch 1/5\n",
      "1709/1709 [==============================] - 204s 119ms/step - loss: 3.6177 - acc: 0.4511 - val_loss: 0.5769 - val_acc: 0.8510\n",
      "Epoch 2/5\n",
      "1709/1709 [==============================] - 74s 43ms/step - loss: 0.1383 - acc: 0.9655 - val_loss: 0.3442 - val_acc: 0.9234\n",
      "Epoch 3/5\n",
      "1664/1709 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9958"
     ]
    }
   ],
   "source": [
    "convolutional = Sequential()\n",
    "\n",
    "convolutional.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3],)))\n",
    "convolutional.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "convolutional.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "convolutional.add(Dropout(0.25))\n",
    "\n",
    "convolutional.add(Flatten())\n",
    "convolutional.add(Dense(128, activation='relu'))\n",
    "convolutional.add(Dropout(0.5))\n",
    "convolutional.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "convolutional.summary()\n",
    "convolutional.compile(loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "evaluate_model(convolutional, 128, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dbb88e68f313fff1eb8dbb54c10fb6dca34e312"
   },
   "source": [
    "From the above epochs, I can already see that my model is over-fitting on my training data from the 2nd epoch on – we can see that the model's testing accuracy is a full 13% higher than its validation accuracy (~93% and ~80%, respectively). The highest validation accuracy comes at the 3rd epoch (~83%), and goes slightly lower in subsequent epochs. \n",
    "\n",
    "Overall, my model is reliably performing at ~81% accuracy, give or take ~2% percent, depending on the model run and epoch. I will now be attempting to optimize this model. \n",
    "\n",
    "## Optimizing the CNN Model\n",
    "### Strategy 0 – Increase Dropout Rates to Counter Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6cf840a87323d52d2ed65f62968b1467db9c8907",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convolutional = Sequential()\n",
    "\n",
    "convolutional.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3],)))\n",
    "convolutional.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "convolutional.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "convolutional.add(Dropout(0.6))\n",
    "\n",
    "convolutional.add(Flatten())\n",
    "convolutional.add(Dense(128, activation='relu'))\n",
    "convolutional.add(Dropout(0.6))\n",
    "convolutional.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "convolutional.summary()\n",
    "convolutional.compile(loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "evaluate_model(convolutional, 128, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f6e1a3110107eb4c6314cc3b41e70c4e03791ff9"
   },
   "source": [
    "Increasing the dropout does help with overfitting – it is still overfitting in the 2nd epoch by ~12%, but that overfit percentage goes down to 5% in the 3rd epoch. Overall, this model seems to be hovering at ~84% validation accuracy.\n",
    "\n",
    "### Strategy 1 – Use Different Loss Functions [Not Successful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5bfce997dd56a47338f7f46ccc252a5674bb2ae1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_with_loss(loss):\n",
    "    convolutional = Sequential()\n",
    "\n",
    "    convolutional.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3],)))\n",
    "    convolutional.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    convolutional.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    convolutional.add(Dropout(0.25))\n",
    "\n",
    "    convolutional.add(Flatten())\n",
    "    convolutional.add(Dense(128, activation='relu'))\n",
    "    convolutional.add(Dropout(0.5))\n",
    "    convolutional.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "    convolutional.summary()\n",
    "    convolutional.compile(loss=loss, optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "    evaluate_model(convolutional, 128, 5)\n",
    "\n",
    "losses = ['mean_squared_error', 'mean_absolute_error', 'mean_squared_logarithmic_error']\n",
    "\n",
    "for loss in losses:\n",
    "    run_with_loss(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "742f372d722112e0831969fab9225600e9be67a0"
   },
   "source": [
    "None of these other loss functions show a significant improvement over the original loss function of 'categorical_crossentropy'. Of the 3 attempted, 'mean_squared_error' performed the best, coming in with a validation accuracy of ~73% before overfitting.\n",
    "\n",
    "### Strategy 2 – Add More Convolutional Layers [Successful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a73a517d2a924dd975f81d944e4f099d1af55f2b",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convolutional_2 = Sequential()\n",
    "\n",
    "convolutional_2.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3],)))\n",
    "convolutional_2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "convolutional_2.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "convolutional_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "convolutional_2.add(Dropout(0.6))\n",
    "\n",
    "# CHANGE\n",
    "# convolutional_2.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "# convolutional_2.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "# convolutional_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# convolutional_2.add(Dropout(0.25))\n",
    "\n",
    "convolutional_2.add(Flatten())\n",
    "convolutional_2.add(Dense(256, activation='relu'))\n",
    "convolutional_2.add(Dropout(0.6))\n",
    "convolutional_2.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "convolutional_2.summary()\n",
    "convolutional_2.compile(loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "evaluate_model(convolutional_2, 128, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c56fe2dc77e51bd8e7f9fb8f628bf1a09f27c4d8"
   },
   "source": [
    "### Strategy 3 – Flatten Before Dropping Out [Not Successful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "28f34a987d91d95742c29d3d1daf66a870e9c277",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convolutional_3 = Sequential()\n",
    "\n",
    "convolutional_3.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3],)))\n",
    "convolutional_3.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "convolutional_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# CHANGE\n",
    "convolutional_3.add(Flatten())\n",
    "convolutional_3.add(Dropout(0.25))\n",
    "\n",
    "convolutional_3.add(Dense(128, activation='relu'))\n",
    "convolutional_3.add(Dropout(0.5))\n",
    "convolutional_3.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "convolutional_3.summary()\n",
    "convolutional_3.compile(loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "evaluate_model(convolutional_3, 128, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6858a8beb6a880880fe00745e1e0b989c48efafd"
   },
   "source": [
    "### Strategy 4 – Change Batch Size (Increase & Decrease)  [Not Successful, Not Successful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a104abe9b1a68096f0b9c398c9fbda1cc8ea02b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convolutional_4 = Sequential()\n",
    "\n",
    "convolutional_4.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3],)))\n",
    "convolutional_4.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "convolutional_4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "convolutional_4.add(Dropout(0.25))\n",
    "\n",
    "convolutional_4.add(Flatten())\n",
    "convolutional_4.add(Dense(128, activation='relu'))\n",
    "convolutional_4.add(Dropout(0.5))\n",
    "convolutional_4.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "convolutional_4.summary()\n",
    "convolutional_4.compile(loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# CHANGE\n",
    "evaluate_model(convolutional_4, 512, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2465377b8155b88527cd783a1ef01029cc0a9e4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convolutional_4_v2 = Sequential()\n",
    "\n",
    "convolutional_4_v2.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3],)))\n",
    "convolutional_4_v2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "convolutional_4_v2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "convolutional_4_v2.add(Dropout(0.25))\n",
    "\n",
    "convolutional_4_v2.add(Flatten())\n",
    "convolutional_4_v2.add(Dense(128, activation='relu'))\n",
    "convolutional_4_v2.add(Dropout(0.5))\n",
    "convolutional_4_v2.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "convolutional_4_v2.summary()\n",
    "convolutional_4_v2.compile(loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# CHANGE\n",
    "evaluate_model(convolutional_4_v2, 32, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4e2deb29a2cdf8253eb84d368c93d3656b793415"
   },
   "source": [
    "### Strategy 5 – Increase & Decrease Kernel Size [Not Successful, Successful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "511810f470015b371fe2ca37fa7dc0708b28ac2d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convolutional_5 = Sequential()\n",
    "\n",
    "# CHANGE\n",
    "convolutional_5.add(Conv2D(32, kernel_size=(4,4), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3],)))\n",
    "\n",
    "convolutional_5.add(Conv2D(64, (4, 4), activation='relu'))\n",
    "convolutional_5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "convolutional_5.add(Dropout(0.25))\n",
    "\n",
    "convolutional_5.add(Flatten())\n",
    "convolutional_5.add(Dense(128, activation='relu'))\n",
    "convolutional_5.add(Dropout(0.5))\n",
    "convolutional_5.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "convolutional_5.summary()\n",
    "convolutional_5.compile(loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "evaluate_model(convolutional_5, 128, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0aec3b2062401111ac566e471ca49c7063b41ba6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convolutional_5_v2 = Sequential()\n",
    "\n",
    "# CHANGE\n",
    "convolutional_5_v2.add(Conv2D(32, kernel_size=(2,2), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3],)))\n",
    "\n",
    "convolutional_5_v2.add(Conv2D(64, (2, 2), activation='relu'))\n",
    "convolutional_5_v2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "convolutional_5_v2.add(Dropout(0.25))\n",
    "\n",
    "convolutional_5_v2.add(Flatten())\n",
    "convolutional_5_v2.add(Dense(128, activation='relu'))\n",
    "convolutional_5_v2.add(Dropout(0.5))\n",
    "convolutional_5_v2.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "convolutional_5_v2.summary()\n",
    "convolutional_5_v2.compile(loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "evaluate_model(convolutional_5_v2, 128, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a11848de250cea34a0cca05ec5c06cd35a4a4275"
   },
   "source": [
    "### Strategy 6 – Add Max Pooling Layer Between Convolutional Layers [Not Successful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5eb613e057de14588c19192525a7284d99a43004",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convolutional_6 = Sequential()\n",
    "\n",
    "convolutional_6.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3],)))\n",
    "convolutional_6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "convolutional_6.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "convolutional_6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "convolutional_6.add(Dropout(0.25))\n",
    "\n",
    "convolutional_6.add(Flatten())\n",
    "convolutional_6.add(Dense(128, activation='relu'))\n",
    "convolutional_6.add(Dropout(0.5))\n",
    "convolutional_6.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "convolutional_6.summary()\n",
    "convolutional_6.compile(loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "evaluate_model(convolutional_6, 128, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e60b45c9edc7b815f9c471d1d460dc136ea96aaf"
   },
   "source": [
    "### Strategy 7 – Adjust Learning Rate of Optimizer [Successful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "86a5fd71e4e08e4325800997b7837d222887b474",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convolutional_7 = Sequential()\n",
    "\n",
    "convolutional_7.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3],)))\n",
    "convolutional_7.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "convolutional_7.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "convolutional_7.add(Dropout(0.25))\n",
    "\n",
    "convolutional_7.add(Flatten())\n",
    "convolutional_7.add(Dense(128, activation='relu'))\n",
    "convolutional_7.add(Dropout(0.5))\n",
    "convolutional_7.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "convolutional_7.summary()\n",
    "convolutional_7.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=.0001), metrics=['accuracy'])\n",
    "\n",
    "evaluate_model(convolutional_7, 128, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd271283aef652bf18d9338b189912b737c9f9c7"
   },
   "source": [
    "### Strategy 8 – Increase Dropout Rates [Successful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94f926482f3990e12976b5bf78887d9b0d779b0c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convolutional_8_v2 = Sequential()\n",
    "\n",
    "convolutional_8_v2.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3],)))\n",
    "convolutional_8_v2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "convolutional_8_v2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "convolutional_8_v2.add(Dropout(0.5))\n",
    "\n",
    "convolutional_8_v2.add(Flatten())\n",
    "convolutional_8_v2.add(Dense(128, activation='relu'))\n",
    "convolutional_8_v2.add(Dropout(0.6))\n",
    "convolutional_8_v2.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "convolutional_8_v2.summary()\n",
    "convolutional_8_v2.compile(loss=\"categorical_crossentropy\", optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "evaluate_model(convolutional_8_v2, 128, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a960b5510a4b6b692331ff37430bc5a89ff4754"
   },
   "source": [
    "## Finalizing Model\n",
    "\n",
    "Add all strategies that worked (additional convolutional layers, smaller learning rate, smaller kernel size), alongside Batch Normalization, for a final optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a38969c880f3a180ab834e90de9f0adb0d3f973",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convolutional_final = Sequential()\n",
    "\n",
    "convolutional_final.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3],)))\n",
    "convolutional_final.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "convolutional_final.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "convolutional_final.add(Dropout(0.4))\n",
    "\n",
    "# CHANGE\n",
    "convolutional_final.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "convolutional_final.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "convolutional_final.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "convolutional_final.add(Dropout(0.4))\n",
    "\n",
    "convolutional_final.add(Flatten())\n",
    "convolutional_final.add(Dense(512, activation='relu'))\n",
    "convolutional_final.add(Dropout(0.6))\n",
    "convolutional_final.add(BatchNormalization())\n",
    "convolutional_final.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "convolutional_final.summary()\n",
    "\n",
    "# CHANGE\n",
    "convolutional_final.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.005), metrics=['accuracy'])\n",
    "\n",
    "# CHANGE\n",
    "evaluate_model(convolutional_final, 128, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e17362a46e9505cd81570167707ec1520c609eb8"
   },
   "source": [
    "* \t\tdrop out layers (add and remove; adjust percentage)\n",
    "* \t\tplay with convolutional layer numbers\n",
    "* \t\t100 epochs for 10,000 images\n",
    "\n",
    "tensor board – good for visualizing model performance (good to add to deep learning portfolio)\n",
    "do visualizations that are similar to what is shown in tensorboard\n",
    "\n",
    "### Evaluating Finalized Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "38586ae80982c0317e226abb59ab143928b3d007",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = convolutional_final.predict(new_X_test, batch_size=None, verbose=0, steps=None).argmax(axis=-1)\n",
    "res_crosstab = pd.crosstab(y_pred, y_test)\n",
    "\n",
    "dict_idx_fruit = {idx: label for idx, label in enumerate(test_labels)}\n",
    "print(dict_idx_fruit)\n",
    "\n",
    "res_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d473cc9ed50f7fdc0d78d3f058329cf08324c1a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx in range(num_categories):\n",
    "    accuracy = res_crosstab.loc[idx, idx] / res_crosstab.loc[:, idx].sum()\n",
    "    flag = '***LOW***' if accuracy < 0.8 else ''\n",
    "    print(dict_idx_fruit[idx])\n",
    "    print('   ', flag, 'accuracy –', round(accuracy * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "59c3cb68ca1a033e56d5eae0a106a7461630bb6b",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
