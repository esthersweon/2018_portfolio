{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook\n",
    "\n",
    "Build and demonstrate a data science product. Reference other scripts as needed, but be sure to include those in the same repo. \n",
    "\n",
    "Demonstrate your technical prowess as well as visualization and narrative storytelling. It should include all stages of your process in an easy-to-read format.\n",
    "\n",
    "- Wrangle your data. Get it into the notebook in the best form possible for your analysis and model building.\n",
    "- Explore your data. Make visualizations and conduct statistical analyses to explain what’s happening with your data, why it’s interesting, and what features you intend to take advantage of for your modeling.\n",
    "- Build a modeling pipeline – build model in a coherent pipeline of linked stages that is efficient and easy to implement.\n",
    "- Evaluate your models. You should have built multiple models, which you should thoroughly evaluate and compare via a robust analysis of residuals and failures\n",
    "- Present and thoroughly explain your product. Describe your model in detail: why you chose it, why it works, what problem it solves, how it will run in a production like environment. What would you need to do to maintain it going forward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Data Science Product\n",
    "\n",
    "- __Problem:__\n",
    "    \n",
    "    Grocery stores must manually tape a barcode to their produce (which may or may not stay on), or have their cashiers memorize particular items' codes, in order for produce to be rung up properly. For self-checkout customers, they have to go through the hassle of looking the items' codes up in the store inventory, and there is nothing saying that they will choose correctly.\n",
    "\n",
    "- __Solution & Value:__\n",
    "    \n",
    "    Grocery stores could use image detection technology to identify items being scanned. This can save cashiers time, prevent customers from selecting an incorrect code, and streamline the self-checkout process – all of which would save the grocery store money.\n",
    "    \n",
    "- __Data Source:__\n",
    "\n",
    "    There is a Kaggle dataset of fruit images for classification that I will use as a starting point. I can supplement this with more fruit images, as well as other produce images (vegetables, herbs, etc.), from scraping Google Image search results.\n",
    "\n",
    "- __Technique:__\n",
    "\n",
    "    I will use deep-learning techniques (i.e. neural networks via Keras / TensorFlow) to categorize images as various produce items.\n",
    "\n",
    "- __Production Environment Deployment:__\n",
    "\n",
    "    The model I create would live on a server, and be fed a photo taken at the register via web protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "  0%|          | 0/166 [00:00<?, ?it/s]/anaconda3/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "100%|██████████| 166/166 [00:00<00:00, 319.08it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 361.37it/s]\n",
      "100%|██████████| 246/246 [00:00<00:00, 349.37it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 363.76it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 362.29it/s]\n",
      "100%|██████████| 165/165 [00:00<00:00, 360.20it/s]\n",
      "100%|██████████| 143/143 [00:00<00:00, 352.56it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 349.75it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 346.83it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 334.91it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 310.50it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 290.32it/s]\n",
      "100%|██████████| 156/156 [00:00<00:00, 348.04it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 316.23it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 360.68it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 359.93it/s]\n",
      "100%|██████████| 165/165 [00:00<00:00, 360.00it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 349.00it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 343.20it/s]\n",
      "100%|██████████| 167/167 [00:00<00:00, 366.89it/s]\n",
      "100%|██████████| 247/247 [00:00<00:00, 348.59it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 363.39it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 354.16it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 354.65it/s]\n",
      "100%|██████████| 161/161 [00:00<00:00, 358.31it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 349.72it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 366.07it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 367.87it/s]\n",
      "100%|██████████| 246/246 [00:00<00:00, 346.15it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 362.23it/s]\n",
      "100%|██████████| 151/151 [00:00<00:00, 363.65it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 345.19it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 361.97it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 353.95it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 351.57it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 352.86it/s]\n",
      "100%|██████████| 246/246 [00:00<00:00, 356.55it/s]\n",
      "100%|██████████| 163/163 [00:00<00:00, 357.54it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 361.94it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 361.70it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 349.17it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 355.69it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 349.06it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 354.94it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 353.95it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 347.57it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 358.34it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 357.33it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 361.46it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 359.78it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 364.91it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 355.37it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 350.88it/s]\n",
      "100%|██████████| 162/162 [00:00<00:00, 363.85it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 356.18it/s]\n",
      "100%|██████████| 165/165 [00:00<00:00, 368.20it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 363.68it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 373.26it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 353.41it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 359.70it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 348.21it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 352.50it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 359.93it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 360.78it/s]\n",
      "100%|██████████| 144/144 [00:00<00:00, 372.19it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 362.97it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 356.92it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 349.03it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 356.08it/s]\n",
      "100%|██████████| 160/160 [00:00<00:00, 360.46it/s]\n",
      "100%|██████████| 166/166 [00:00<00:00, 352.71it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 346.75it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 348.09it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 343.82it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 351.63it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 340.32it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 362.58it/s]\n",
      "100%|██████████| 738/738 [00:02<00:00, 340.79it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 342.65it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 342.06it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 354.48it/s]\n",
      "100%|██████████| 427/427 [00:01<00:00, 357.69it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 356.18it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 360.82it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 362.22it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 358.23it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 362.15it/s]\n",
      "100%|██████████| 466/466 [00:01<00:00, 364.06it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 366.69it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 361.09it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 361.57it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 362.35it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 360.00it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 359.81it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 363.24it/s]\n",
      "100%|██████████| 738/738 [00:02<00:00, 360.70it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 357.57it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 358.97it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 359.84it/s]\n",
      "100%|██████████| 481/481 [00:01<00:00, 359.62it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 355.31it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 364.02it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 359.70it/s]\n",
      "100%|██████████| 738/738 [00:02<00:00, 356.11it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 367.60it/s]\n",
      "100%|██████████| 447/447 [00:01<00:00, 370.31it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 366.92it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 367.61it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 356.16it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 363.05it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 354.23it/s]\n",
      "100%|██████████| 738/738 [00:02<00:00, 359.28it/s]\n",
      "100%|██████████| 493/493 [00:01<00:00, 361.47it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 362.45it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 356.96it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 361.21it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 357.31it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 344.62it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 355.43it/s]\n",
      "100%|██████████| 491/491 [00:01<00:00, 349.00it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 347.23it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 351.51it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 343.76it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 353.46it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 366.06it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 361.18it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 347.72it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 353.73it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 354.11it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 352.04it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 355.50it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 365.56it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 363.48it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 369.11it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 369.66it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 366.13it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 353.44it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 341.72it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 331.69it/s]\n",
      "100%|██████████| 429/429 [00:01<00:00, 252.16it/s]\n",
      "100%|██████████| 735/735 [00:02<00:00, 252.81it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 255.45it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 263.55it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 349.83it/s]\n",
      "100%|██████████| 479/479 [00:01<00:00, 317.07it/s]\n",
      "100%|██████████| 490/490 [00:01<00:00, 310.10it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 324.29it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 300.80it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 318.63it/s]\n",
      "100%|██████████| 492/492 [00:01<00:00, 320.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import skimage\n",
    "from skimage import io, transform\n",
    "from tqdm import tqdm\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "img_size = 100\n",
    "train_dir = './data/fruits/test_single/'\n",
    "test_dir =  './data/fruits/train_single/'\n",
    "\n",
    "def get_data(folder_path):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for idx, folder_name in enumerate(os.listdir(folder_path)):\n",
    "        if not folder_name.startswith('.'):\n",
    "            for file_name in tqdm(os.listdir(folder_path + folder_name)):\n",
    "                if not file_name.startswith('.'):\n",
    "                    img_file = io.imread(folder_path + folder_name + '/' + file_name)\n",
    "                    if img_file is not None:\n",
    "                        img_file = transform.resize(img_file, (img_size, img_size))\n",
    "                        imgs.append(np.asarray(img_file))\n",
    "                        labels.append(idx)\n",
    "    imgs = np.asarray(imgs)\n",
    "    labels = np.asarray(labels)\n",
    "    return imgs, labels\n",
    "\n",
    "X_train, y_train = get_data(train_dir)\n",
    "X_test, y_test = get_data(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (12709, 100, 100, 3)\n",
      "X_test shape: (37836, 100, 100, 3)\n",
      "y_train: [ 0  0  0 ... 74 74 74]\n",
      "y_test: [ 0  0  0 ... 74 74 74]\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train:', y_train)\n",
    "print('y_test:', y_test)\n",
    "\n",
    "num_categories = len(np.unique(y_train))\n",
    "\n",
    "new_y_train = keras.utils.to_categorical(y_train, num_categories)\n",
    "new_y_test = keras.utils.to_categorical(y_test, num_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "### Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               3000100   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 75)                7575      \n",
      "=================================================================\n",
      "Total params: 3,017,775\n",
      "Trainable params: 3,017,775\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp_model = Sequential()\n",
    "\n",
    "mlp_model.add(Dense(100, activation='relu', input_shape=(X_train.shape[1] * X_train.shape[2] * X_train.shape[3],)))\n",
    "mlp_model.add(Dropout(0.1))\n",
    "mlp_model.add(Dense(100, activation='relu'))\n",
    "mlp_model.add(Dropout(0.1))\n",
    "mlp_model.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "mlp_model.summary()\n",
    "mlp_model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 98, 98, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 96, 96, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 147456)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               18874496  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 75)                9675      \n",
      "=================================================================\n",
      "Total params: 18,903,563\n",
      "Trainable params: 18,903,563\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "convolutional_model = Sequential()\n",
    "\n",
    "convolutional_model.add(Conv2D(32, kernel_size=(X_train.shape[3], X_train.shape[3]), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3],)))\n",
    "convolutional_model.add(Conv2D(64, (X_train.shape[3], X_train.shape[3]), activation='relu'))\n",
    "convolutional_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "convolutional_model.add(Dropout(0.25))\n",
    "convolutional_model.add(Flatten())\n",
    "convolutional_model.add(Dense(128, activation='relu'))\n",
    "convolutional_model.add(Dropout(0.5))\n",
    "convolutional_model.add(Dense(num_categories, activation='softmax'))\n",
    "\n",
    "convolutional_model.summary()\n",
    "convolutional_model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Why you chose a model, why it works, what problem it solves, how it will run in production-like environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, new_X_train, new_X_test, batch_size, epochs):\n",
    "    history = model.fit(new_X_train, new_y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(new_X_test, new_y_test))\n",
    "    score = model.evaluate(new_X_test, new_y_test, verbose=0)\n",
    "    print('***Loss***', score[0])\n",
    "    print('***Accuracy***', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12709 samples, validate on 37836 samples\n",
      "Epoch 1/10\n",
      "12709/12709 [==============================] - 31s 2ms/step - loss: 4.8329 - acc: 0.0265 - val_loss: 4.0452 - val_acc: 0.0325\n",
      "Epoch 2/10\n",
      "12709/12709 [==============================] - 14s 1ms/step - loss: 4.0245 - acc: 0.0328 - val_loss: 3.9560 - val_acc: 0.0325\n",
      "Epoch 3/10\n",
      "12709/12709 [==============================] - 9s 734us/step - loss: 3.9652 - acc: 0.0324 - val_loss: 3.9097 - val_acc: 0.0361\n",
      "Epoch 4/10\n",
      "12709/12709 [==============================] - 9s 683us/step - loss: 3.9050 - acc: 0.0320 - val_loss: 3.8480 - val_acc: 0.0356\n",
      "Epoch 5/10\n",
      "12709/12709 [==============================] - 8s 634us/step - loss: 3.8517 - acc: 0.0353 - val_loss: 3.8434 - val_acc: 0.0323\n",
      "Epoch 6/10\n",
      "12709/12709 [==============================] - 8s 629us/step - loss: 3.8043 - acc: 0.0393 - val_loss: 3.7609 - val_acc: 0.0406\n",
      "Epoch 7/10\n",
      "12709/12709 [==============================] - 8s 616us/step - loss: 3.7788 - acc: 0.0417 - val_loss: 3.7430 - val_acc: 0.0525\n",
      "Epoch 8/10\n",
      "12709/12709 [==============================] - 9s 675us/step - loss: 3.7186 - acc: 0.0529 - val_loss: 3.6934 - val_acc: 0.0602\n",
      "Epoch 9/10\n",
      "12709/12709 [==============================] - 8s 643us/step - loss: 3.7077 - acc: 0.0508 - val_loss: 3.6607 - val_acc: 0.0557\n",
      "Epoch 10/10\n",
      "12709/12709 [==============================] - 8s 637us/step - loss: 3.6807 - acc: 0.0547 - val_loss: 3.6413 - val_acc: 0.0540\n",
      "***Loss*** 3.641265244347334\n",
      "***Accuracy*** 0.05399619410085633\n"
     ]
    }
   ],
   "source": [
    "new_X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2] * X_train.shape[3]).astype('float32')\n",
    "new_X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2] * X_test.shape[3]).astype('float32')\n",
    "evaluate_model(mlp_model, new_X_train, new_X_test, 150, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12709 samples, validate on 37836 samples\n",
      "Epoch 1/10\n",
      "12709/12709 [==============================] - 627s 49ms/step - loss: 3.9816 - acc: 0.0830 - val_loss: 2.8361 - val_acc: 0.3309\n",
      "Epoch 2/10\n",
      "12709/12709 [==============================] - 565s 44ms/step - loss: 2.2696 - acc: 0.3757 - val_loss: 1.6441 - val_acc: 0.6156\n",
      "Epoch 3/10\n",
      "12709/12709 [==============================] - 542s 43ms/step - loss: 1.1319 - acc: 0.6573 - val_loss: 1.2871 - val_acc: 0.7039\n",
      "Epoch 4/10\n",
      "12709/12709 [==============================] - 547s 43ms/step - loss: 0.6667 - acc: 0.7865 - val_loss: 1.2034 - val_acc: 0.7287\n",
      "Epoch 5/10\n",
      "12709/12709 [==============================] - 546s 43ms/step - loss: 0.4855 - acc: 0.8356 - val_loss: 1.2279 - val_acc: 0.7535\n",
      "Epoch 6/10\n",
      "12709/12709 [==============================] - 530s 42ms/step - loss: 0.3999 - acc: 0.8670 - val_loss: 1.2705 - val_acc: 0.7552\n",
      "Epoch 7/10\n",
      "12709/12709 [==============================] - 531s 42ms/step - loss: 0.3382 - acc: 0.8841 - val_loss: 1.2997 - val_acc: 0.7677\n",
      "Epoch 8/10\n",
      "12709/12709 [==============================] - 560s 44ms/step - loss: 0.2860 - acc: 0.9016 - val_loss: 1.2784 - val_acc: 0.7655\n",
      "Epoch 9/10\n",
      "12709/12709 [==============================] - 540s 43ms/step - loss: 0.2510 - acc: 0.9112 - val_loss: 1.2689 - val_acc: 0.7797\n",
      "Epoch 10/10\n",
      "12709/12709 [==============================] - 536s 42ms/step - loss: 0.2193 - acc: 0.9205 - val_loss: 1.4427 - val_acc: 0.7761\n",
      "***Loss*** 1.4427357740878168\n",
      "***Accuracy*** 0.7760862670480443\n"
     ]
    }
   ],
   "source": [
    "new_X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3]).astype('float32')\n",
    "new_X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], X_test.shape[3]).astype('float32')\n",
    "evaluate_model(convolutional_model, new_X_train, new_X_test, 150, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
